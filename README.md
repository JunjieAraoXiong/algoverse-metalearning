# Self-Correcting RAG for Financial Document QA

A multi-agent RAG framework with judge-driven self-correction for high-stakes document QA.

**Paper**: Self-Correcting RAG: Judge-Driven Retrieval for Financial Document QA (FINAI@ICLR 2026)

## ğŸ”‘ Key Features

- **Three Specialized Agents**: Retrieval, Reasoning, and Judge agents with escalation strategies
- **Self-Correction Loop**: Judge-driven retry when answers are below quality threshold
- **Rule-Based Routing**: Zero-cost pipeline selection matching LLM-based routers
- **Cross-Domain**: Works on Finance (FinanceBench), Medical (PubMedQA), and Legal (CUAD)

## ğŸ“ Project Structure

```
rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # â­ Multi-agent system (Algorithm 1)
â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # Main retry loop
â”‚   â”‚   â”œâ”€â”€ retrieval_agent.py
â”‚   â”‚   â”œâ”€â”€ reasoning_agent.py
â”‚   â”‚   â””â”€â”€ judge_agent.py
â”‚   â”œâ”€â”€ retrieval_tools/     # Retrieval pipelines
â”‚   â”‚   â”œâ”€â”€ semantic.py      # Dense vector search
â”‚   â”‚   â”œâ”€â”€ hybrid.py        # BM25 + semantic ensemble
â”‚   â”‚   â”œâ”€â”€ rerank.py        # Cross-encoder reranking
â”‚   â”‚   â”œâ”€â”€ hyde.py          # Hypothetical Document Embeddings
â”‚   â”‚   â””â”€â”€ router.py        # Question-type routing
â”‚   â”œâ”€â”€ providers/           # LLM adapters (OpenAI, Anthropic, etc.)
â”‚   â”œâ”€â”€ meta_learning/       # Router training
â”‚   â”œâ”€â”€ config.py            # Central configuration
â”‚   â””â”€â”€ bulk_testing.py      # Evaluation entry point
â”œâ”€â”€ evaluation/              # Metrics & LLM-as-Judge
â”œâ”€â”€ dataset_adapters/        # FinanceBench, PubMedQA, CUAD loaders
â””â”€â”€ scripts/                 # Experiment & training scripts
```

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/JunjieAraoXiong/algoverse-metalearning.git
cd algoverse-metalearning/rag
pip install -r requirements.txt
cp .env.example .env  # Add your API keys (OPENAI_API_KEY, ANTHROPIC_API_KEY)
```

### Run Single-Pass Baseline
```bash
python src/bulk_testing.py \
    --dataset financebench \
    --pipeline hybrid_filter_rerank \
    --model gpt-4o-mini \
    --use-llm-judge
```

### Run Self-Correcting RAG (Agentic Mode)
```bash
python src/bulk_testing.py \
    --dataset financebench \
    --pipeline routed \
    --model gpt-4o-mini \
    --use-llm-judge \
    --use-agentic-retry \
    --max-retries 1
```

## ğŸ”§ Retrieval Pipelines

| Pipeline | Description | When to Use |
|----------|-------------|-------------|
| `semantic` | Dense vector search | Simple factual queries |
| `hybrid` | BM25 + semantic ensemble | Keyword-heavy queries |
| `hybrid_filter` | Hybrid + metadata filtering | Entity-specific queries |
| `hybrid_filter_rerank` | Full pipeline with reranking | Complex reasoning |
| `routed` | Dynamic selection + retry | **Production (recommended)** |

## ğŸ§  Algorithm Overview

The orchestrator implements a judge-driven retry loop:

```
while attempt â‰¤ max_retries:
    docs = RetrievalAgent.retrieve(question, attempt)
    answer = ReasoningAgent.generate(question, docs)
    score = JudgeAgent.evaluate(question, answer)

    if score â‰¥ threshold:
        return answer

    escalate_strategies()
    attempt += 1
```

**Escalation strategies:**
- **Retrieval**: Increase k (10â†’20â†’25), enable HyDE
- **Reasoning**: Standard â†’ Conservative â†’ Detailed prompts
- **Judge**: Lower threshold (0.5â†’0.4â†’0.3)

## ğŸ“Š Reproducing Results

### Step 1: Prepare ChromaDB
```bash
# Download pre-built embeddings (recommended)
# [Link to be added after publication]

# Or build from scratch (requires SEC PDFs)
python src/ingest_docling.py --input-dir data/pdfs --output-dir chroma_docling
```

### Step 2: Run Experiments
```bash
# Table 1: Single-pass vs Self-Correcting RAG
python src/bulk_testing.py --dataset financebench --pipeline hybrid_filter_rerank --model gpt-4o-mini --use-llm-judge
python src/bulk_testing.py --dataset financebench --pipeline routed --model gpt-4o-mini --use-llm-judge --use-agentic-retry

# Table 4: Cross-domain validation
python src/bulk_testing.py --dataset pubmedqa --pipeline routed --model gpt-4o-mini --use-agentic-retry --domain medical
python src/bulk_testing.py --dataset cuad --pipeline routed --model gpt-4o-mini --use-agentic-retry --domain legal
```

## ğŸ›  Supported Models

| Provider | Models | Notes |
|----------|--------|-------|
| OpenAI | gpt-4o-mini, gpt-4o | Recommended for evaluation |
| Anthropic | claude-sonnet-4-5-20250514 | High quality |
| Together | Llama 3.1 70B | For cluster deployment |

## ğŸ“„ Citation

```bibtex
@inproceedings{anonymous2026selfcorrecting,
  title={Self-Correcting RAG: Judge-Driven Retrieval for Financial Document QA},
  author={Anonymous},
  booktitle={FINAI Workshop at ICLR 2026},
  year={2026}
}
```

## ğŸ“ License

MIT License
