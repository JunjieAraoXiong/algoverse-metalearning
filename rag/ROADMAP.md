# Financial RAG ‚Üí Meta-Learning Roadmap

---

## Work Completed Summary

### Foundation Work (Done Before Phases)

| Work Item | Time Spent | What Was Done | Files Created/Modified |
|-----------|------------|---------------|------------------------|
| **Project Structure** | ~2 hrs | Set up directory structure, config patterns, coding guidelines | `.claude/CLAUDE.md`, `NOTES.md` |
| **Central Configuration** | ~1 hr | Centralized all defaults, model configs, embedding configs | `src/config.py` |
| **LLM Provider Abstraction** | ~1.5 hrs | Abstract base class + factory for multiple providers | `src/providers/*.py` (5 files) |
| **Retrieval Pipeline Framework** | ~2 hrs | Modular retrieval with semantic, hybrid, filter, rerank | `src/retrieval_tools/*.py` (6 files) |
| **Evaluation Framework** | ~1 hr | Bulk testing runner with metrics and reporting | `src/bulk_testing.py`, `evaluation/` |
| **Dataset Adapters** | ~1 hr | Pluggable dataset loaders for FinanceBench, PubMedQA | `dataset_adapters/*.py` |
| **PDF Download** | ~30 min | Downloaded 367/368 FinanceBench PDFs (636 MB) | `scripts/download_financebench_pdfs.py` |
| **Documentation** | ~1 hr | Project notes, guidelines, this roadmap | `NOTES.md`, `ROADMAP.md` |

**Total Foundation Work: ~10 hours**

### What's Built and Working

```
‚úÖ COMPLETE                    ‚ö†Ô∏è NEEDS WORK                 üî≤ NOT STARTED
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚úÖ Project structure           ‚ö†Ô∏è ChromaDB (old, no metadata) üî≤ Meta-router
‚úÖ Config system               ‚ö†Ô∏è Table detection             üî≤ Oracle labels
‚úÖ Provider abstraction        ‚ö†Ô∏è Question classifier         üî≤ PubMedQA setup
‚úÖ Retrieval pipelines                                        üî≤ CUAD setup
‚úÖ Reranker integration
‚úÖ Bulk testing framework
‚úÖ Metadata extraction (filename)
‚úÖ V2 ingestion script (untested on full set)
‚úÖ 367 PDFs downloaded
```

---

## System Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FINANCIAL RAG SYSTEM                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   INGEST    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   STORE     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  RETRIEVE   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  GENERATE   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (PDFs)     ‚îÇ    ‚îÇ  (ChromaDB) ‚îÇ    ‚îÇ  (Pipeline) ‚îÇ    ‚îÇ   (LLM)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ          ‚îÇ
‚îÇ        ‚ñº                  ‚ñº                  ‚ñº                  ‚ñº          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇUnstructured‚îÇ     ‚îÇ BGE-Large ‚îÇ      ‚îÇ  Hybrid   ‚îÇ      ‚îÇClaude 4.5 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  hi_res   ‚îÇ      ‚îÇEmbeddings ‚îÇ      ‚îÇ +Filter   ‚îÇ      ‚îÇ GPT 5.2   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  +OCR     ‚îÇ      ‚îÇ  (FREE)   ‚îÇ      ‚îÇ +Rerank   ‚îÇ      ‚îÇ Gemini 3  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Notes: RAG Architecture

**What is RAG?**
- **R**etrieval-**A**ugmented **G**eneration - instead of asking an LLM to answer from memory, we first retrieve relevant documents, then generate an answer grounded in those documents
- Reduces hallucination because the LLM has real source material
- Enables answering questions about private/recent data not in training set

**Why these 4 stages?**
1. **Ingest** - PDFs aren't searchable. We must extract text, split into chunks, and create searchable representations
2. **Store** - Vector databases (ChromaDB) enable fast similarity search over millions of chunks
3. **Retrieve** - Find the most relevant chunks for a question (this is where most errors happen!)
4. **Generate** - LLM synthesizes an answer from retrieved context

**Why local embeddings (BGE) instead of OpenAI?**
- OpenAI charges per token for BOTH ingestion AND every query
- 367 PDFs √ó ~500 chunks √ó 2 passes = expensive
- BGE-large is comparable quality and completely FREE (runs locally)
- Trade-off: slightly slower, uses local CPU/GPU

**Why multiple LLM providers?**
- Different models have different strengths (Claude for reasoning, Gemini for speed, DeepSeek for cost)
- Provider abstraction lets us swap models without changing code
- Enables A/B testing different models on same questions

---

## What We Built (Detailed Component Map)

```
rag/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    ‚úÖ BUILT - Central configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EMBEDDINGS               - 6 models (4 free local, 2 paid OpenAI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PROVIDERS                - 5 LLM providers (OpenAI, Anthropic, Google, Together, DeepSeek)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RERANKERS                - 3 reranker options
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DEFAULTS                 - All default settings
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ providers/                   ‚úÖ BUILT - LLM abstraction layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                  - Abstract LLMProvider class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py               - get_provider(model_name) with caching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py       - GPT-4o, GPT-5.2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_provider.py    - Claude 4.5 Sonnet/Opus
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ google_provider.py       - Gemini 3 Flash/Pro
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval_tools/             ‚úÖ BUILT - Retrieval pipelines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool_registry.py         - Pipeline builder & registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic.py              - Pure vector similarity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid.py                - BM25 + Semantic (50/50)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_filter.py       - Filter by company/year
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rerank.py                - Cross-encoder reranking (BGE)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ metadata_utils.py            ‚úÖ BUILT - Metadata extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parse_filename()         - Extract company/year/doc_type from PDF name
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract_metadata_from_question()  - Extract from questions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filter_chunks_by_metadata()       - Filter retrieved chunks
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ create_database.py           ‚úÖ BUILT - Basic ingestion
‚îÇ   ‚îú‚îÄ‚îÄ create_database_element_based.py     ‚úÖ BUILT - Element-aware ingestion
‚îÇ   ‚îú‚îÄ‚îÄ create_database_v2.py        ‚úÖ BUILT - Improved ingestion with metadata
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ bulk_testing.py              ‚úÖ BUILT - Evaluation framework
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BulkTestConfig           - Configuration dataclass
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BulkTestRunner           - Main runner class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_single_question()- RAG pipeline execution
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_bulk_test()          - Batch evaluation
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ meta_learning/               üî≤ STUB - Not yet implemented
‚îÇ       ‚îú‚îÄ‚îÄ router.py
‚îÇ       ‚îú‚îÄ‚îÄ oracle_labels.py
‚îÇ       ‚îú‚îÄ‚îÄ episodes.py
‚îÇ       ‚îú‚îÄ‚îÄ meta_trainer.py
‚îÇ       ‚îî‚îÄ‚îÄ evaluator.py
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                      ‚úÖ BUILT - Metrics
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   - embedding_similarity, aggregate_metrics
‚îÇ   ‚îî‚îÄ‚îÄ llm_judge.py                 - LLM-as-a-Judge evaluation
‚îÇ
‚îú‚îÄ‚îÄ dataset_adapters/                ‚úÖ BUILT - Dataset loaders
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      - BaseDatasetAdapter
‚îÇ   ‚îú‚îÄ‚îÄ financebench.py              - FinanceBench loader
‚îÇ   ‚îî‚îÄ‚îÄ pubmedqa.py                  - PubMedQA loader
‚îÇ
‚îú‚îÄ‚îÄ data/test_files/
‚îÇ   ‚îî‚îÄ‚îÄ finance-bench-pdfs/          ‚úÖ 367 PDFs downloaded (636 MB)
‚îÇ
‚îî‚îÄ‚îÄ chroma/                          ‚ö†Ô∏è  OLD - Needs rebuild with v2 ingestion
```

---

## Retrieval Pipeline Flow (What Happens on Each Question)

```
                              RETRIEVAL PIPELINE
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  Question: "What is 3M's FY2018 capital expenditure?"                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 1: INITIAL RETRIEVAL (k √ó factor = 5 √ó 3 = 15 chunks)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   BM25      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  Semantic   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (keyword)  ‚îÇ    ‚îÇ    ‚îÇ  (vector)   ‚îÇ    ‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                     ‚ñº                       ‚ñº                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ     HYBRID MERGE (50/50)     ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ     ‚Üí 15 candidate chunks    ‚îÇ                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 2: METADATA FILTER                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Question ‚Üí extract_metadata() ‚Üí {company: "3M", year: 2018}        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  15 chunks ‚Üí filter(company="3M", year=2018) ‚Üí 8 chunks             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚ö†Ô∏è  CURRENT ISSUE: Chunk metadata incomplete (no company/year)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ FIX: create_database_v2.py adds this metadata                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 3: RERANK (Cross-Encoder)                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Model: BAAI/bge-reranker-large                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  8 chunks ‚Üí CrossEncoder(question, chunk) ‚Üí score ‚Üí top 5           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Output: 5 most relevant chunks                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 4: GENERATION                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Context = concat(5 chunks)                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Prompt = system_prompt + context + question                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  LLM = Claude 4.5 Sonnet (or GPT-5.2, Gemini 3)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Answer: "$1,577 million"                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Notes: Retrieval Pipeline

**Why retrieve more than we need (k √ó factor)?**
- If we want 5 final chunks, we retrieve 15 first (factor = 3)
- This gives filtering and reranking room to work
- Without over-retrieval, filtering might leave us with 0 relevant chunks

**What is BM25?**
- Classic keyword-based search algorithm (like Google circa 2000)
- Matches exact terms: "FY2018" matches "FY2018" but not "fiscal year 2018"
- Fast, interpretable, great for specific terms like company names and years
- Weakness: misses synonyms and semantic similarity

**What is Semantic (Vector) Search?**
- Embeds question and chunks into high-dimensional vectors
- Finds chunks with similar "meaning" even if words differ
- "What is the revenue?" matches "Total sales were $X million"
- Weakness: can miss exact keyword matches, sometimes retrieves vaguely similar but wrong content

**Why Hybrid (50/50)?**
- Combines strengths of both: BM25 catches exact matches, semantic catches meaning
- Research shows hybrid consistently outperforms either alone
- The 50/50 weight is a reasonable default; could be tuned per domain

**What is a Cross-Encoder Reranker?**
- Takes (question, chunk) pairs and scores relevance 0-1
- Much more accurate than embedding similarity but ~100x slower
- That's why we only rerank top 15, not all 50,000 chunks
- BGE-reranker-large is SOTA for English, runs locally (FREE)

**Why is metadata filtering so important?**
- FinanceBench questions are specific: "3M's FY2018 CapEx"
- Without filtering, we might retrieve Adobe's 2019 data instead
- Per-file RAG (51%) vs shared-store RAG (19%) shows 2.7x improvement just from filtering!

---

## Available Retrieval Pipelines

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        PIPELINE OPTIONS                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  1. semantic                                                                ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                           ‚îÇ
‚îÇ     ‚îÇ Semantic ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Results            ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                           ‚îÇ
‚îÇ     Pure vector similarity. Fast, but misses keyword matches.              ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  2. hybrid                                                                  ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ
‚îÇ     ‚îÇ   BM25   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Merge   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Results            ‚îÇ
‚îÇ     ‚îÇ Semantic ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  50/50   ‚îÇ                                           ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ
‚îÇ     Combines keyword + semantic. Better recall.                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  3. hybrid_filter                                                           ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ     ‚îÇ  Hybrid  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Metadata ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Top-K   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Results            ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  Filter  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            ‚îÇ
‚îÇ     Filters by company/year before taking top-K.                           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  4. hybrid_filter_rerank  ‚Üê DEFAULT (RECOMMENDED)                          ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ     ‚îÇ  Hybrid  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Metadata ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Rerank   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Results            ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  Filter  ‚îÇ   ‚îÇ(CrossEnc)‚îÇ                            ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ     Best quality. Reranker scores relevance precisely.                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Notes: Pipeline Selection

**When to use each pipeline:**

| Pipeline | Best For | Latency | Quality |
|----------|----------|---------|---------|
| `semantic` | Quick prototyping, simple questions | ~50ms | Low |
| `hybrid` | General use, mixed question types | ~100ms | Medium |
| `hybrid_filter` | Domain-specific with clear metadata | ~120ms | High |
| `hybrid_filter_rerank` | Production, accuracy-critical | ~300ms | Highest |

**Why is `hybrid_filter_rerank` the default?**
- Financial questions have clear metadata (company, year) making filtering effective
- Reranking catches subtle relevance that embedding similarity misses
- The 200ms extra latency is acceptable for accuracy-critical applications
- For real-time chat, might drop to `hybrid_filter` to reduce latency

**Trade-offs:**
- More stages = higher accuracy but slower
- Reranking is the biggest latency hit (~200ms for 15 chunks)
- Filtering requires good metadata; if metadata is wrong, it hurts instead of helps

---

## Ingestion Pipeline (create_database_v2.py)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INGESTION FLOW (V2)                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  INPUT: 3M_2018_10K.pdf                                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 1: PARSE FILENAME                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  "3M_2018_10K.pdf" ‚Üí parse_filename() ‚Üí                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  {                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    company: "3M",                                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    year: 2018,                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    doc_type: "10K",                                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    fiscal_period: "FY2018"                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  }                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 2: PDF PARSING (Unstructured.io)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  partition_pdf(                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    strategy="hi_res",         # High-quality OCR                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    infer_table_structure=True # Detect tables                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  )                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Elements: [Title, Text, Table, Text, Table, ...]                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 3: SEMANTIC CHUNKING                                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  chunk_by_title(                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    max_characters=2000,                                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    combine_text_under_n_chars=1000                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  )                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Groups content by section headers                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Keeps tables intact (doesn't split mid-table)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 4: ENRICH METADATA                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Each chunk gets:                                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  {                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    company: "3M",              # From filename                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    year: 2018,                 # From filename                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    doc_type: "10K",            # From filename                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    fiscal_period: "FY2018",    # Derived                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    element_type: "table",      # From Unstructured                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    page_number: 45,            # From Unstructured                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    source_file: "3M_2018_10K.pdf"                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  }                                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                        ‚îÇ
‚îÇ                                    ‚ñº                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ STEP 5: EMBED & STORE                                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Model: BAAI/bge-large-en-v1.5 (FREE, local)                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Store: ChromaDB (persistent)                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  367 PDFs ‚Üí ~50,000+ chunks ‚Üí ChromaDB                              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Notes: Ingestion

**Why Unstructured.io with `hi_res` strategy?**
- Financial documents have complex layouts: tables, multi-column text, headers/footers
- `hi_res` uses OCR + layout detection to properly extract tables
- Alternative `fast` strategy just extracts text linearly (loses table structure)
- Trade-off: `hi_res` is ~10x slower but much better for financial docs

**Why `chunk_by_title` instead of fixed-size chunking?**
- Fixed 1000-char chunks can split mid-sentence or mid-table
- `chunk_by_title` respects document structure (sections, headers)
- Tables stay intact as single chunks (critical for financial data!)
- Produces semantically coherent chunks that embed better

**Why extract metadata from filenames?**
- FinanceBench PDFs follow pattern: `COMPANY_YEAR_DOCTYPE.pdf`
- Parsing this gives us structured metadata for filtering
- Without it, we can only match on text content (less precise)
- Example: `3M_2018_10K.pdf` ‚Üí `{company: "3M", year: 2018, doc_type: "10K"}`

**What is ChromaDB?**
- Open-source vector database (like Pinecone but free/local)
- Stores embeddings + metadata + original text
- Supports filtering by metadata fields
- Persists to disk so we don't re-embed every time

**Embedding dimension matters:**
- BGE-large: 1024 dimensions
- Higher dimensions = more expressive but larger storage
- 50,000 chunks √ó 1024 dims √ó 4 bytes = ~200MB (manageable)

---

## Current State

| Metric | Our Score | Target |
|--------|-----------|--------|
| Semantic Similarity | 0.495 | 0.65+ |
| metrics-generated | 0.35 | 0.55+ |
| domain-relevant | 0.60 | 0.70+ |
| novel-generated | 0.53 | 0.65+ |

## Benchmark Context (FinanceBench)

| Approach | Accuracy | Notes |
|----------|----------|-------|
| Baseline RAG (shared-store) | 19% | GPT-4-Turbo, 81% wrong/refused |
| Improved RAG (Ragie) | 27% | Hybrid search, better ingestion |
| Per-file RAG (single-store) | 51% | Retrieve from correct document only |
| Long-context (100k+ tokens) | ~70-80% | Expensive, high latency |

**Key insight**: Per-file retrieval (51%) is 2.7x better than shared-store (19%). Our goal is to match or beat per-file RAG accuracy.

### Technical Notes: FinanceBench Benchmark

**What is FinanceBench?**
- 10,231 questions about publicly traded companies from 10-K/10-Q filings
- 150 questions with human-annotated answers (what we test on)
- Questions span: numerical extraction, analysis, multi-hop reasoning
- Created by Patronus AI to test RAG systems on real financial documents

**Why is it hard?**
- Financial documents are dense: 100+ page 10-Ks with complex tables
- Questions require precise numerical answers ("$1,577 million" not "$1.5 billion")
- Wrong company/year data is worse than no answer (hallucination risk)
- Many questions need calculation (ratios, YoY changes)

**What do the accuracy numbers mean?**
- **19% baseline**: Just embedding search + GPT-4 = 81% wrong/refused
- **51% per-file**: When you give the correct document only, accuracy jumps 2.7x
- **70-80% long-context**: Feed entire 100-page doc into 100k context window (expensive!)

**Our evaluation metric: Semantic Similarity**
- We measure embedding similarity between predicted and gold answers
- 0.495 avg score ‚âà "sometimes correct, often partially correct"
- Not the same as accuracy (binary right/wrong) but correlates
- Scores by question type reveal where we fail (metrics-generated: 0.35)

---

---

## Required Reading: FinanceBench Repository

Before diving deeper, familiarize yourself with the benchmark:

**GitHub:** https://github.com/patronus-ai/financebench

| File/Folder | What It Contains | Priority |
|-------------|------------------|----------|
| `README.md` | Overview, data loading code, evaluation methodology | ‚≠ê **READ FIRST** |
| `data/financebench_open_source.jsonl` | 150 annotated QA pairs with answers + evidence | ‚≠ê **Essential** |
| `data/financebench_document_information.jsonl` | Document metadata (which PDFs map to which companies) | ‚≠ê **Essential** |
| `evaluation_playground.ipynb` | Interactive notebook to explore the data | Helpful |
| `results/` | Their model evaluation results (GPT-4, Claude, etc.) | Reference |
| `pdfs/` | Source PDFs (we already downloaded 367 of these) | Already have |
| [arXiv Paper](https://arxiv.org/abs/2311.11944) | Full methodology, all results, analysis | ‚≠ê **Read for context** |

**Key code from their README to load data:**
```python
import json
questions = [json.loads(line) for line in open("financebench_open_source.jsonl")]
doc_info = [json.loads(line) for line in open("financebench_document_information.jsonl")]

# Each question has:
# - question: the question text
# - answer: gold answer
# - evidence: text from source doc
# - doc_name: which PDF it comes from
# - question_type: metrics-generated, domain-relevant, or novel-generated
```

---

## Phase 1: Improved Ingestion (Current)

**Status**: 90% complete | **Estimated Time Remaining**: ~2-4 hours (mostly waiting for ingestion)

### What This Phase Accomplishes
Transform raw PDFs into a searchable vector database with rich metadata, enabling accurate filtering by company/year during retrieval.

### Completed Tasks ‚úÖ

| Task | Time Spent | What Was Done | Files Modified |
|------|------------|---------------|----------------|
| Design metadata schema | ~30 min | Defined `DocumentMetadata` dataclass with company, year, doc_type, quarter, fiscal_period | `src/metadata_utils.py` |
| Implement filename parser | ~20 min | `parse_filename()` extracts metadata from `COMPANY_YEAR_DOCTYPE.pdf` pattern | `src/metadata_utils.py` |
| Create v2 ingestion script | ~1 hr | Full pipeline: parse filename ‚Üí Unstructured hi_res ‚Üí chunk_by_title ‚Üí enrich metadata ‚Üí embed ‚Üí store | `src/create_database_v2.py` |
| Add table‚Üímarkdown conversion | ~20 min | `html_table_to_markdown()` converts Unstructured's HTML tables to markdown | `src/create_database_v2.py` |
| Add element type tagging | ~10 min | `get_element_type()` maps Unstructured elements to table/prose/title/etc | `src/create_database_v2.py` |
| Install OCR dependencies | ~10 min | `brew install poppler tesseract` for hi_res PDF parsing | System |
| Test on sample PDF | ~15 min | Verified 486 chunks from 3M_2016_10K.pdf with correct metadata | Tested |

### Remaining Tasks ‚¨ú

| Task | Estimated Time | What To Do | Command/Notes |
|------|----------------|------------|---------------|
| Run full ingestion | 2-4 hrs (mostly waiting) | Process all 367 PDFs through v2 pipeline | `python src/create_database_v2.py` |
| Verify metadata coverage | ~15 min | Check all chunks have company/year metadata | Query ChromaDB, spot check |
| (Optional) Tune table detection | ~1 hr | Currently 0 tables detected; may need to adjust Unstructured params | Investigate `infer_table_structure` |

### Verified Output (from test run)
```
Metadata: {
  'company': '3M',
  'year': 2016,
  'doc_type': '10K',
  'fiscal_period': 'FY2016',
  'element_type': 'other',
  'source_file': '3M_2016_10K.pdf'
}
```

### Key Files
| File | Purpose |
|------|---------|
| `src/create_database_v2.py` | Main ingestion script (run this) |
| `src/metadata_utils.py` | Filename parsing + question metadata extraction |
| `src/create_database.py` | Old basic ingestion (deprecated) |
| `src/create_database_element_based.py` | Previous attempt (superseded by v2) |

### How To Run
```bash
# Test on small sample first
python src/create_database_v2.py --sample 5

# Full ingestion (will take 2-4 hours)
python src/create_database_v2.py

# Check output
ls -la chroma/  # Should see new database files
```

---

## Phase 2: Per-File Retrieval Strategy

**Status**: 0% complete | **Estimated Time**: 3-4 hours

### What This Phase Accomplishes
Implement strict metadata filtering so questions about "3M FY2018" only retrieve chunks from the 3M 2018 10-K, not from other companies or years.

### Why This Matters
- Per-file RAG (51%) vs shared-store RAG (19%) = **2.7x improvement**
- This is the single biggest lever for accuracy improvement
- Most RAG errors come from retrieving irrelevant context

### Strategy A: Metadata-Filtered Retrieval (Recommended)
```
Question ‚Üí Extract {company, year} ‚Üí Filter ChromaDB ‚Üí Retrieve ‚Üí Generate
```

### Strategy B: Two-Stage Retrieval (Alternative)
```
Question ‚Üí Stage 1: Identify document ‚Üí Stage 2: Retrieve from document ‚Üí Generate
```

### All Tasks

| Task | Estimated Time | Status | What To Do |
|------|----------------|--------|------------|
| Enhance `extract_metadata_from_question()` | ~1 hr | ‚¨ú | Improve company/year extraction from question text (regex + NER) |
| Update `filter_chunks_by_metadata()` | ~30 min | ‚¨ú | Make filtering stricter, add exact match option |
| Add ChromaDB native filtering | ~1 hr | ‚¨ú | Use `db.similarity_search(filter={"company": "3M", "year": 2018})` |
| Implement fallback strategy | ~30 min | ‚¨ú | If filter returns 0 results, relax constraints progressively |
| Add logging for filter effectiveness | ~20 min | ‚¨ú | Track how many chunks filtered out, catch over-filtering |
| Benchmark per-file vs shared | ~1 hr | ‚¨ú | Run same questions with and without filtering, compare scores |

### Key Files to Modify
| File | Changes Needed |
|------|----------------|
| `src/metadata_utils.py` | Improve `extract_metadata_from_question()` |
| `src/retrieval_tools/metadata_filter.py` | Add strict filtering, ChromaDB native filter |
| `src/retrieval_tools/tool_registry.py` | Wire up new filtering logic |

### How To Test
```bash
# After changes, run on subset
python src/bulk_testing.py --subset data/question_sets/financebench_subset_questions.csv --pipeline hybrid_filter

# Compare scores with and without filtering
python src/bulk_testing.py --pipeline hybrid          # No filter
python src/bulk_testing.py --pipeline hybrid_filter   # With filter
```

### Technical Notes: Per-File Retrieval

**Why does per-file work so much better?**
- Shared-store problem: "What is 3M's revenue?" might retrieve Apple's revenue chunk (similar words!)
- Per-file solution: First identify the document, then search within it
- This mimics how humans search: find the right report, then Ctrl+F within it

**How metadata filtering achieves this:**
```python
# Without filtering (shared-store):
results = db.similarity_search("3M FY2018 revenue", k=5)
# Might return: [Adobe_2019, 3M_2017, Apple_2018, 3M_2018, Microsoft_2018]

# With filtering (per-file equivalent):
results = db.similarity_search(
    "3M FY2018 revenue",
    k=5,
    filter={"company": "3M", "year": 2018}  # ‚Üê This is the key!
)
# Returns: [3M_2018_chunk1, 3M_2018_chunk2, ...] (all from correct doc)
```

**Fallback strategy matters:**
- What if we can't extract company/year from question?
- What if the metadata is wrong?
- Need graceful degradation: try filtered ‚Üí fall back to unfiltered

**Two-stage vs single-stage:**
- Single-stage: Filter ChromaDB directly (what we do now)
- Two-stage: First classify document, then query that document's index
- Two-stage is cleaner but requires separate indices per document (more complex)

---

## Phase 3: Question-Type Routing

**Status**: 0% complete | **Estimated Time**: 4-5 hours

### What This Phase Accomplishes
Route different question types to specialized retrieval/generation strategies. Our weakest area (metrics-generated: 0.35) needs table-aware handling.

### Why This Matters
| Type | Current Score | Target | Gap |
|------|---------------|--------|-----|
| metrics-generated | 0.35 | 0.55+ | **Biggest opportunity** |
| domain-relevant | 0.60 | 0.70+ | Medium |
| novel-generated | 0.53 | 0.65+ | Medium |

### All Tasks

| Task | Estimated Time | Status | What To Do |
|------|----------------|--------|------------|
| Build question classifier | ~1.5 hr | ‚¨ú | Classify question ‚Üí {metrics, domain, novel} using keywords/regex or small model |
| Create metrics-specific prompt | ~45 min | ‚¨ú | Structured extraction: "Find the exact number in the context for X" |
| Create domain-specific prompt | ~30 min | ‚¨ú | Chain-of-thought: "Analyze the data and reason step by step" |
| Create novel-specific prompt | ~30 min | ‚¨ú | Multi-hop: "Consider multiple factors and synthesize" |
| Add table-priority retrieval | ~1 hr | ‚¨ú | For metrics questions, boost chunks with `element_type="table"` |
| Add calculation verification | ~45 min | ‚¨ú | Post-generation check: "Does this number appear in context?" |
| Wire up routing in pipeline | ~30 min | ‚¨ú | Modify `bulk_testing.py` to use classifier ‚Üí select prompt |

### Key Files to Create/Modify
| File | Purpose |
|------|---------|
| `src/question_classifier.py` | **NEW** - Classify question type |
| `src/prompts/` | **NEW** - Directory for prompt templates |
| `src/prompts/metrics_prompt.py` | Numeric extraction prompt |
| `src/prompts/domain_prompt.py` | Analytical reasoning prompt |
| `src/prompts/novel_prompt.py` | Multi-hop synthesis prompt |
| `src/bulk_testing.py` | Add routing logic |

### Example Question Classification
```python
def classify_question(question: str) -> str:
    q_lower = question.lower()

    # Metrics indicators
    if any(w in q_lower for w in ['what is the', 'how much', 'what was the',
                                   'capex', 'revenue', 'ratio', 'margin', '$']):
        return 'metrics-generated'

    # Domain indicators
    if any(w in q_lower for w in ['is it', 'does', 'should', 'capital-intensive',
                                   'healthy', 'risk', 'outlook']):
        return 'domain-relevant'

    # Novel/complex
    if any(w in q_lower for w in ['if we', 'excluding', 'trend', 'compare',
                                   'which segment', 'what drove']):
        return 'novel-generated'

    return 'domain-relevant'  # Default
```

### How To Test
```bash
# Run with question-type breakdown
python src/bulk_testing.py --model claude-sonnet-4-5 --top-k 10

# Check results by question type
# Output CSV will have question_type column for analysis
```

### Technical Notes: Question Types

**Why does question type matter?**
Different questions need different retrieval AND generation strategies:

**1. Metrics-Generated (our weakest: 0.35)**
```
Example: "What is the FY2018 capital expenditure amount for 3M?"
Expected: "$1,577 million"

Why hard:
- Need to find exact table row/cell
- Must get units right (millions vs billions)
- LLM might hallucinate plausible-sounding numbers

Better strategy:
- Prioritize table chunks (filter by element_type="table")
- Use structured extraction prompting
- Verify: "Does this number appear verbatim in context?"
```

**2. Domain-Relevant (decent: 0.60)**
```
Example: "Is 3M a capital-intensive business based on FY2022 data?"
Expected: "Yes, based on high PP&E/Assets ratio of X%..."

Why medium:
- Needs interpretation, not just extraction
- Requires domain knowledge (what makes a business "capital-intensive"?)
- Answer is synthesized, not copied

Better strategy:
- Retrieve more context (longer chunks)
- Use chain-of-thought prompting
- Include domain definitions in prompt
```

**3. Novel-Generated (OK: 0.53)**
```
Example: "If we exclude M&A impact, which segment dragged down 3M's growth?"
Expected: "Safety & Industrial segment, excluding acquisitions..."

Why medium:
- Requires multi-hop reasoning
- May need data from multiple sections
- Counterfactual ("if we exclude...")

Better strategy:
- Multi-query retrieval (rephrase question multiple ways)
- Retrieve from multiple document sections
- Explicit reasoning steps in prompt
```

**Question classifier approach:**
- Train small classifier on question text ‚Üí type
- Or use regex/keyword rules (simpler, often good enough)
- Route to different pipeline/prompt based on type

---

## Phase 4: Evaluation & Validation

**Status**: 0% complete | **Estimated Time**: 2-3 hours

### What This Phase Accomplishes
Rigorously evaluate our improved RAG system, measure accuracy (not just similarity), and document what works.

### Success Criteria
| Metric | Target | Notes |
|--------|--------|-------|
| Overall accuracy | ‚â• 50% | Match per-file RAG baseline |
| metrics-generated | ‚â• 40% | From current 0.35 similarity |
| domain-relevant | ‚â• 65% | From current 0.60 similarity |
| novel-generated | ‚â• 55% | From current 0.53 similarity |

### All Tasks

| Task | Estimated Time | Status | What To Do |
|------|----------------|--------|------------|
| Run full 150-question eval | ~30 min (run time) | ‚¨ú | `python src/bulk_testing.py --model claude-sonnet-4-5` |
| Add accuracy metric | ~30 min | ‚¨ú | Binary correct/incorrect based on gold answer match |
| Analyze by question type | ~30 min | ‚¨ú | Group results by metrics/domain/novel, identify patterns |
| Analyze by company/year | ~20 min | ‚¨ú | Check if certain companies/years perform worse |
| Compare vs baseline | ~20 min | ‚¨ú | Document improvement over initial 0.495 score |
| Document failure modes | ~30 min | ‚¨ú | Categorize errors: wrong doc, wrong number, hallucination, etc. |
| Write evaluation report | ~30 min | ‚¨ú | Summarize findings for future reference |

### Key Commands
```bash
# Full evaluation with best model
python src/bulk_testing.py --model claude-sonnet-4-5 --top-k 10 --pipeline hybrid_filter_rerank

# Quick evaluation with cheaper model (for iteration)
python src/bulk_testing.py --model gemini-3-flash --top-k 10

# Results will be in bulk_runs/ directory
ls bulk_runs/*.csv
ls bulk_runs/*.json  # Summary stats
```

### Evaluation Output Format
```
bulk_runs/
‚îú‚îÄ‚îÄ 2024-12-12_financebench_claude45-sonnet_k10_t0.csv   # Full results
‚îî‚îÄ‚îÄ 2024-12-12_financebench_claude45-sonnet_k10_t0.json  # Summary
```

### How To Analyze Results
```python
import pandas as pd
df = pd.read_csv('bulk_runs/LATEST_RESULTS.csv')

# Overall score
print(f"Mean similarity: {df['semantic_similarity'].mean():.3f}")

# By question type
print(df.groupby('question_type')['semantic_similarity'].mean())

# Worst performing questions
print(df.nsmallest(10, 'semantic_similarity')[['question', 'semantic_similarity']])
```

---

## Phase 5: Meta-Learning Pivot

**Status**: 0% complete | **Estimated Time**: 8-12 hours

### What This Phase Accomplishes
Build a meta-learning system that learns to select the optimal retrieval pipeline for any question, generalizing across Finance, Healthcare, and Legal domains.

### Why This Is Novel (Paper Contribution)
- Most RAG papers optimize ONE pipeline
- We optimize the CHOICE of pipeline
- Cross-domain generalization is underexplored
- Clean experimental setup for reproducibility

---

### Required Reading: Meta-Learning

| Resource | What You'll Learn | Priority |
|----------|-------------------|----------|
| [MAML Paper](https://arxiv.org/abs/1703.03400) | Model-Agnostic Meta-Learning fundamentals | ‚≠ê **Core concept** |
| [Prototypical Networks](https://arxiv.org/abs/1703.05175) | Metric-based meta-learning (simpler than MAML) | ‚≠ê **Recommended approach** |
| [Meta-Learning Survey](https://arxiv.org/abs/2004.05439) | Overview of all meta-learning approaches | Reference |
| [Learn2Learn Library](https://github.com/learnables/learn2learn) | PyTorch meta-learning implementations | Practical |
| Our notes: `src/meta_learning/README.md` | Project-specific meta-learning design | When ready |

**Key concepts to understand:**
- **Episode**: One training iteration (support set + query set)
- **Support set**: Few examples with labels (e.g., 5 questions with best pipeline)
- **Query set**: Questions to predict pipeline for
- **N-way K-shot**: N classes (pipelines), K examples per class

---

### Required Reading: New Domains

#### Healthcare: PubMedQA
| Resource | What It Contains | Priority |
|----------|------------------|----------|
| [PubMedQA Paper](https://arxiv.org/abs/1909.06146) | Dataset description, baselines, methodology | ‚≠ê **Read first** |
| [HuggingFace Dataset](https://huggingface.co/datasets/pubmed_qa) | Direct data access | ‚≠ê **Use this** |
| [PubMed](https://pubmed.ncbi.nlm.nih.gov/) | Source medical literature | Reference |

**PubMedQA characteristics:**
- ~1,000 questions about medical research
- Yes/No/Maybe answers with reasoning
- Dense prose (no tables) - different from Finance!
- Tests comprehension of scientific abstracts

#### Legal: CUAD (Contract Understanding)
| Resource | What It Contains | Priority |
|----------|------------------|----------|
| [CUAD Paper](https://arxiv.org/abs/2103.06268) | Contract Understanding Atticus Dataset | ‚≠ê **Read first** |
| [HuggingFace Dataset](https://huggingface.co/datasets/cuad) | Direct data access | ‚≠ê **Use this** |
| [GitHub Repo](https://github.com/TheAtticusProject/cuad) | Code, examples, evaluation | Reference |

**CUAD characteristics:**
- 510 contracts, 13,000+ annotations
- 41 clause types (termination, liability, IP rights, etc.)
- Extractive QA - find specific clauses
- Tests precise legal text extraction

---

### All Tasks

| Task | Estimated Time | Status | What To Do |
|------|----------------|--------|------------|
| **Setup Phase** |
| Set up PubMedQA dataset | ~1 hr | ‚¨ú | Create `dataset_adapters/pubmedqa.py`, download data |
| Set up CUAD dataset | ~1 hr | ‚¨ú | Create `dataset_adapters/cuad.py`, download data |
| Ingest PubMedQA docs | ~2 hrs | ‚¨ú | Create embeddings for medical abstracts |
| Ingest CUAD contracts | ~2 hrs | ‚¨ú | Create embeddings for legal documents |
| **Oracle Labels** |
| Run grid search on Finance | ~2 hrs | ‚¨ú | Run all 4 pipelines on all 150 questions |
| Run grid search on PubMedQA | ~2 hrs | ‚¨ú | Run all 4 pipelines on PubMedQA questions |
| Run grid search on CUAD | ~2 hrs | ‚¨ú | Run all 4 pipelines on CUAD questions |
| Create oracle label dataset | ~30 min | ‚¨ú | For each question, record best pipeline |
| **Meta-Router** |
| Design router architecture | ~1 hr | ‚¨ú | MLP classifier: question embedding ‚Üí pipeline |
| Implement episodic training | ~2 hrs | ‚¨ú | Support set ‚Üí router state ‚Üí predict query set |
| Train on Finance + Healthcare | ~1 hr | ‚¨ú | Hold out Legal for testing |
| **Evaluation** |
| Evaluate on held-out Legal | ~1 hr | ‚¨ú | Test cross-domain generalization |
| Compare vs fixed pipeline | ~30 min | ‚¨ú | Meta-router vs always-hybrid_filter_rerank |
| Document results | ~1 hr | ‚¨ú | Write up findings for paper |

### Key Files to Create
```
src/meta_learning/
‚îú‚îÄ‚îÄ router.py              # Meta-router model
‚îú‚îÄ‚îÄ oracle_labels.py       # Grid search for best pipeline per question
‚îú‚îÄ‚îÄ episodes.py            # Episodic data sampling
‚îú‚îÄ‚îÄ meta_trainer.py        # Training loop
‚îî‚îÄ‚îÄ evaluator.py           # Cross-domain evaluation

dataset_adapters/
‚îú‚îÄ‚îÄ pubmedqa.py            # PubMedQA loader
‚îî‚îÄ‚îÄ cuad.py                # CUAD loader

chroma_pubmedqa/           # Vector store for medical domain
chroma_cuad/               # Vector store for legal domain
```

### Meta-Learning Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     META-LEARNING FOR RAG                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  TRAINING: Learn which pipeline works best for which question type         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ EPISODE œÑ (one domain, e.g., Finance)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Support Set SœÑ:                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Q1: "What is 3M's FY2018 CapEx?" ‚Üí best: hybrid_filter      ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Q2: "Is 3M capital-intensive?"   ‚Üí best: semantic           ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Q3: "What drove margin change?"  ‚Üí best: hybrid_filter_rerank‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                           ‚ñº                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ              META-ROUTER fœÜ                                  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                              ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Input: question embedding + support set                     ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Output: pipeline_id to use                                  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                              ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Architecture: Transformer or MLP classifier                 ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                           ‚ñº                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Query Set:                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Q4: "What is Adobe's FY2019 revenue?"                       ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     ‚Üí Router predicts: hybrid_filter                        ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     ‚Üí Execute pipeline ‚Üí Evaluate ‚Üí Update router           ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  CROSS-DOMAIN GENERALIZATION:                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ   FINANCE    ‚îÇ    ‚îÇ  HEALTHCARE  ‚îÇ    ‚îÇ    LEGAL     ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ FinanceBench ‚îÇ    ‚îÇ   PubMedQA   ‚îÇ    ‚îÇ     CUAD     ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ 10-K, 10-Q   ‚îÇ    ‚îÇ Medical lit  ‚îÇ    ‚îÇ  Contracts   ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ Tables, nums ‚îÇ    ‚îÇ Dense prose  ‚îÇ    ‚îÇ  Clauses     ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ                          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                             ‚ñº                                              ‚îÇ
‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ                   ‚îÇ   SHARED ROUTER  ‚îÇ                                     ‚îÇ
‚îÇ                   ‚îÇ                  ‚îÇ                                     ‚îÇ
‚îÇ                   ‚îÇ Learns patterns: ‚îÇ                                     ‚îÇ
‚îÇ                   ‚îÇ ‚Ä¢ numeric ‚Üí filter‚îÇ                                    ‚îÇ
‚îÇ                   ‚îÇ ‚Ä¢ dense ‚Üí semantic‚îÇ                                    ‚îÇ
‚îÇ                   ‚îÇ ‚Ä¢ multi-hop ‚Üí rerank                                   ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Domains
1. **Finance** - FinanceBench (current)
2. **Healthcare** - PubMedQA
3. **Legal** - CUAD (Contract Understanding)

### Tasks:
- [ ] Set up PubMedQA dataset and evaluation
- [ ] Set up CUAD dataset and evaluation
- [ ] Implement oracle labels (grid-search per question type)
- [ ] Train meta-router
- [ ] Evaluate cross-domain few-shot adaptation

### Paper Contribution:
- Meta-learned tool selection beats any fixed pipeline
- Cross-domain generalization with few-shot adaptation
- Clean recipe: grid-search ‚Üí oracle ‚Üí train router ‚Üí episodic eval

### Technical Notes: Meta-Learning

**What is meta-learning?**
- "Learning to learn" - instead of learning one task, learn how to quickly adapt to new tasks
- Classic example: recognize new animal species from 5 examples (few-shot learning)
- Our application: learn which RAG pipeline works best for which question type

**Why meta-learning for RAG?**
- Different domains have different optimal pipelines
- Finance: tables, numbers ‚Üí needs filtering
- Medical: dense prose ‚Üí semantic might be better
- Legal: specific clauses ‚Üí keyword matching important
- Instead of manually tuning per domain, learn the pattern

**What are "oracle labels"?**
- Ground truth for "which pipeline is best for this question"
- Created by grid search: run all pipelines on each question, pick winner
- Example:
  ```
  Q: "What is 3M's CapEx?"
  - semantic: 0.3 score
  - hybrid: 0.4 score
  - hybrid_filter: 0.7 score  ‚Üê winner
  - hybrid_filter_rerank: 0.65 score
  Oracle label: "hybrid_filter"
  ```

**Episodic training:**
- Sample a "task" (e.g., a domain or question type)
- Show router a few examples with oracle labels (support set)
- Ask router to predict pipeline for new question (query set)
- Update router based on how well it predicts

**Cross-domain generalization:**
- Train on Finance + Medical ‚Üí test on Legal (unseen domain)
- If router learns abstract patterns ("numeric questions need filtering")
- It should generalize without seeing Legal training data
- This is the key contribution for a paper!

**Why this is novel:**
- Most RAG papers optimize one pipeline
- We're saying: optimize the CHOICE of pipeline
- Meta-learning is underexplored in RAG literature
- Clean experimental setup: 3 domains, 4 pipelines, episodic evaluation

---

## Progress Tracker

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           PROJECT PROGRESS                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  FOUNDATION (Pre-Phase Work)                           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%   ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Project structure & guidelines                                      ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Central configuration (config.py)                                   ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ LLM provider abstraction (5 providers)                              ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Retrieval pipelines (4 strategies)                                  ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Evaluation framework                                                ‚îÇ
‚îÇ  ‚îî‚îÄ ‚úÖ 367 PDFs downloaded                                                 ‚îÇ
‚îÇ  Time spent: ~10 hours                                                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  PHASE 1: Improved Ingestion                           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 90%    ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Metadata schema design                          (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Filename parser                                 (~20 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ V2 ingestion script                             (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Table‚Üímarkdown conversion                       (~20 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Element type tagging                            (~10 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ OCR dependencies (poppler, tesseract)           (~10 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚úÖ Sample test (486 chunks verified)               (~15 min)           ‚îÇ
‚îÇ  ‚îî‚îÄ ‚¨ú Full ingestion (367 PDFs)                       (~2-4 hrs waiting)  ‚îÇ
‚îÇ  Time spent: ~2.5 hrs | Remaining: ~2-4 hrs                                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  PHASE 2: Per-File Retrieval                           [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0%     ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Enhance metadata extraction                     (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Update filter logic                             (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú ChromaDB native filtering                       (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Fallback strategy                               (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Filter logging                                  (~20 min)           ‚îÇ
‚îÇ  ‚îî‚îÄ ‚¨ú Benchmark comparison                            (~1 hr)             ‚îÇ
‚îÇ  Estimated: ~4 hrs                                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  PHASE 3: Question-Type Routing                        [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0%     ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Question classifier                             (~1.5 hrs)          ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Metrics-specific prompt                         (~45 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Domain-specific prompt                          (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Novel-specific prompt                           (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Table-priority retrieval                        (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Calculation verification                        (~45 min)           ‚îÇ
‚îÇ  ‚îî‚îÄ ‚¨ú Wire up routing                                 (~30 min)           ‚îÇ
‚îÇ  Estimated: ~5 hrs                                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  PHASE 4: Evaluation & Validation                      [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0%     ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Full 150-question eval                          (~30 min run)       ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Add accuracy metric                             (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Analyze by question type                        (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Analyze by company/year                         (~20 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Compare vs baseline                             (~20 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Document failure modes                          (~30 min)           ‚îÇ
‚îÇ  ‚îî‚îÄ ‚¨ú Write evaluation report                         (~30 min)           ‚îÇ
‚îÇ  Estimated: ~3 hrs                                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  PHASE 5: Meta-Learning Pivot                          [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 0%     ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú PubMedQA dataset setup                          (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú CUAD dataset setup                              (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Ingest PubMedQA                                 (~2 hrs)            ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Ingest CUAD                                     (~2 hrs)            ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Grid search (3 domains √ó 4 pipelines)           (~6 hrs)            ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Create oracle labels                            (~30 min)           ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Design router architecture                      (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Implement episodic training                     (~2 hrs)            ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Train on Finance + Healthcare                   (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Evaluate on Legal (cross-domain)                (~1 hr)             ‚îÇ
‚îÇ  ‚îú‚îÄ ‚¨ú Compare vs fixed pipeline                       (~30 min)           ‚îÇ
‚îÇ  ‚îî‚îÄ ‚¨ú Document results                                (~1 hr)             ‚îÇ
‚îÇ  Estimated: ~12 hrs                                                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  TIME SUMMARY                                                               ‚îÇ
‚îÇ  ‚îú‚îÄ Foundation (complete):     ~10 hrs                                     ‚îÇ
‚îÇ  ‚îú‚îÄ Phase 1 (90% complete):    ~2.5 hrs done, ~3 hrs remaining             ‚îÇ
‚îÇ  ‚îú‚îÄ Phase 2 (0%):              ~4 hrs estimated                            ‚îÇ
‚îÇ  ‚îú‚îÄ Phase 3 (0%):              ~5 hrs estimated                            ‚îÇ
‚îÇ  ‚îú‚îÄ Phase 4 (0%):              ~3 hrs estimated                            ‚îÇ
‚îÇ  ‚îî‚îÄ Phase 5 (0%):              ~12 hrs estimated                           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  TOTAL: ~12.5 hrs done | ~27 hrs remaining                                 ‚îÇ
‚îÇ  OVERALL PROGRESS: [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] ~32%                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  NEXT ACTION: Run full ingestion                                           ‚îÇ
‚îÇ  COMMAND: python src/create_database_v2.py                                 ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Quick Reference

### Commands
```bash
# Test improved ingestion
python src/create_database_v2.py --sample 5

# Full ingestion
python src/create_database_v2.py

# Run evaluation
python src/bulk_testing.py --model claude-sonnet-4-5 --top-k 10

# Run on subset
python src/bulk_testing.py --subset data/question_sets/financebench_subset_questions.csv
```

### Cost Estimates (150 questions)
| Model | Cost/Run |
|-------|----------|
| Claude 4.5 Sonnet | ~$2.00 |
| GPT 5.2 | ~$3.20 |
| Gemini 3 Flash | ~$0.07 |
| DeepSeek Chat | ~$0.05 |

### Key Files
| File | Purpose |
|------|---------|
| `src/config.py` | Centralized configuration |
| `src/create_database_v2.py` | Improved ingestion |
| `src/bulk_testing.py` | Evaluation runner |
| `src/retrieval_tools/` | Retrieval pipelines |
| `src/meta_learning/` | Meta-learning (stubs) |

---

## Timeline (Effort-Based, No Dates)

| Phase | Effort | Dependencies |
|-------|--------|--------------|
| Phase 1 | 2-3 sessions | None |
| Phase 2 | 3-4 sessions | Phase 1 |
| Phase 3 | 2-3 sessions | Phase 2 |
| Phase 4 | 1-2 sessions | Phase 3 |
| Phase 5 | 5-7 sessions | Phase 4 |

---

*Last updated: December 12, 2024*

---

## Glossary

| Term | Definition |
|------|------------|
| **RAG** | Retrieval-Augmented Generation - retrieve docs, then generate answer |
| **Embedding** | Dense vector representation of text (~1024 dimensions) |
| **ChromaDB** | Open-source vector database for storing/searching embeddings |
| **BM25** | Classic keyword-based search algorithm |
| **Cross-Encoder** | Model that scores (query, doc) pairs for relevance |
| **Reranker** | Cross-encoder used to re-order retrieved documents |
| **Chunk** | A segment of text from a document (~1000-2000 chars) |
| **10-K** | Annual SEC filing (comprehensive financial report) |
| **10-Q** | Quarterly SEC filing (less detailed than 10-K) |
| **Semantic Similarity** | Cosine similarity between embeddings (0-1) |
| **Meta-learning** | Learning to learn - adapting quickly to new tasks |
| **Episode** | One training iteration in meta-learning (support + query sets) |
| **Oracle Label** | Ground truth best pipeline for a question (from grid search) |

---

## Sources
- [FinanceBench Paper](https://arxiv.org/abs/2311.11944)
- [Ragie FinanceBench Results](https://www.ragie.ai/blog/ragie-outperformed-financebench)
- [Databricks Long Context RAG](https://www.databricks.com/blog/long-context-rag-performance-llms)
- [Patronus AI FinanceBench Docs](https://docs.patronus.ai/docs/research_and_differentiators/financebench)
