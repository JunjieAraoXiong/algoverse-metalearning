% Results Section for Self-Correcting RAG Paper
% FINAI@ICLR 2026 Workshop

\section{Results}
\label{sec:results}

% TODO: Fill in 0.XX placeholders with actual experimental results

\subsection{FinanceBench Results (Primary Evaluation)}

Table~\ref{tab:financebench-main} presents our primary results on FinanceBench, the core benchmark for this work.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Sem. Sim.} & \textbf{LLM Judge} & \textbf{Numeric Acc.} \\
\midrule
Single-pass (best baseline) & 0.XX & 0.XX & 0.XX \\
Self-Correcting RAG & 0.XX & 0.XX & 0.XX \\
\midrule
\textit{Improvement} & +X.X\% & +X.X\% & +X.X\% \\
\bottomrule
\end{tabular}
\caption{FinanceBench results comparing single-pass RAG against Self-Correcting RAG. Numeric Acc.\ measures exact-match accuracy on questions requiring numerical extraction or calculation.}
\label{tab:financebench-main}
\end{table}

While the absolute accuracy gain of X\% may appear modest compared to Self-Refine's reported $\sim$20\% improvements on general reasoning benchmarks, this comparison overlooks two critical factors. First, our baseline performance is substantially higher (0.XX vs.\ typical 40--60\% baselines), meaning each remaining error represents a harder question. Second, framed as error reduction, our X\% absolute gain translates to a Y\% reduction in error rate, a meaningful improvement in high-stakes financial applications where reliability is paramount.

\paragraph{Performance by Question Type.}
Table~\ref{tab:financebench-breakdown} breaks down results by FinanceBench question category:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Question Type} & \textbf{Single-Pass} & \textbf{Multi-Agent} & \textbf{$\Delta$} & \textbf{Num. Acc.} \\
\midrule
Metrics-generated & 0.XX & 0.XX & +X.X\% & 0.XX \\
Domain-relevant & 0.XX & 0.XX & +X.X\% & 0.XX \\
Novel-generated & 0.XX & 0.XX & +X.X\% & 0.XX \\
\bottomrule
\end{tabular}
\caption{FinanceBench performance by question type. Metrics-generated questions (numerical extraction) show largest improvement from self-correction. Num.\ Acc.\ reports numerical accuracy for the Multi-Agent configuration.}
\label{tab:financebench-breakdown}
\end{table}

\paragraph{Numerical Reasoning Gap.}
Financial question-answering requires precise numerical reasoning that exposes fundamental limitations in single-pass RAG. On FinanceBench, baseline accuracy on questions requiring numerical extraction or calculation (0.XX) lags significantly behind simpler factual extraction questions (0.XX), a gap of X percentage points. Self-Correcting RAG narrows this gap substantially: numerical accuracy improves from 0.XX to 0.XX (+X.X\%), while factual extraction shows more modest gains (+X.X\%). This pattern confirms that self-correction provides the greatest benefit precisely where single-pass approaches struggle most: questions requiring arithmetic operations, unit conversions, or multi-step numerical reasoning.

\paragraph{Temporal Reasoning.}
Questions requiring cross-fiscal-year comparisons present unique challenges in financial document QA. We identify X/X FinanceBench questions that require temporal reasoning (e.g., year-over-year growth, multi-period comparisons). On these questions, single-pass RAG achieves only 0.XX accuracy, with common errors including: (1)~conflating fiscal year (FY) with calendar year dates, (2)~extracting figures from incorrect reporting periods, and (3)~failing to align quarterly vs.\ annual data. The Judge Agent proves particularly effective at detecting temporal misalignment errors, flagging X\% of such mistakes for retry. After retry, temporal reasoning accuracy improves to 0.XX, demonstrating that self-correction can address this challenging error category.

\textbf{Key FinanceBench findings:}
\begin{itemize}[nosep]
    \item Largest gains on \emph{metrics-generated} questions requiring numerical precision, exactly where single-pass RAG most frequently fails
    \item Self-correction recovers X\% of initially incorrect answers through escalated retrieval and more careful prompting
    \item The Judge Agent's score correlates with answer quality, providing reliable uncertainty estimates for downstream decision-making
\end{itemize}

\subsection{Ablation Study}

Table~\ref{tab:ablation} shows the contribution of each component on FinanceBench.

\begin{table}[h]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{FinanceBench} \\
\midrule
Full Self-Correcting RAG & 0.XX \\
\quad $-$ Judge Agent (no retry) & 0.XX \\
\quad $-$ Retrieval Agent routing & 0.XX \\
Single-pass baseline & 0.XX \\
\bottomrule
\end{tabular}
\caption{Ablation study on FinanceBench. Each row removes one component.}
\label{tab:ablation}
\end{table}

\subsection{Self-Correction Analysis}
\label{sec:correction-analysis}

The central claim of Self-Correcting RAG is \emph{resilience}: the ability to recover from initial failures. We quantify this through detailed analysis of the correction flow.

\paragraph{Correction Flow.}
Table~\ref{tab:correction-flow} presents the complete self-correction flow on FinanceBench. We introduce the \textbf{Lazarus Rate}: the percentage of initially incorrect answers that are successfully corrected through retry. This metric directly measures system resilience.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Count} & \textbf{Rate} \\
\midrule
Total Questions & 150 & -- \\
Correct on Attempt 1 & XX & XX\% \\
Incorrect on Attempt 1 & XX & XX\% \\
\midrule
Triggered Retry (Judge flagged) & XX & XX\% of incorrect \\
Corrected by Retry & XX & \textbf{XX\%} (Lazarus Rate) \\
Still Wrong after Retry & XX & XX\% \\
Not Flagged (missed by Judge) & XX & XX\% \\
\bottomrule
\end{tabular}
\caption{Self-correction flow analysis on FinanceBench. The ``Lazarus Rate'' measures what percentage of initially incorrect answers were successfully corrected through retry---a direct measure of system resilience.}
\label{tab:correction-flow}
\end{table}

The Lazarus Rate of XX\% demonstrates that self-correction provides meaningful recovery: nearly half of initial failures are successfully corrected. This is the core value proposition---not merely ``better performance,'' but \emph{demonstrated ability to recover from mistakes}.

\paragraph{Judge Discrimination.}
For self-correction to work, the Judge Agent must reliably distinguish correct from incorrect answers. We evaluate Judge discrimination using standard classification metrics:

\begin{itemize}[nosep]
    \item \textbf{True Positive Rate (Sensitivity)}: XX\% of wrong answers correctly flagged for retry
    \item \textbf{False Positive Rate}: XX\% of correct answers unnecessarily flagged
    \item \textbf{Precision}: XX\% of flagged answers were actually wrong
    \item \textbf{F1 Score}: 0.XX (harmonic mean of precision and recall)
\end{itemize}

The high True Positive Rate (>XX\%) indicates the Judge successfully identifies most failures, while the low False Positive Rate (<XX\%) means correct answers rarely trigger unnecessary retry overhead. This discrimination enables efficient self-correction: the system retries when needed without wasting resources on already-correct answers.

\paragraph{Disproportionate Numeric Gain.}
A key finding is that self-correction provides \emph{disproportionate} benefit on numerical questions---precisely where single-pass RAG struggles most:

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Single-Pass} & \textbf{Self-Correcting} & \textbf{$\Delta$} \\
\midrule
Semantic Similarity & 0.XX & 0.XX & +X.X\% \\
\textbf{Numeric Accuracy} & 0.XX & 0.XX & \textbf{+XX.X\%} \\
\bottomrule
\end{tabular}
\caption{Disproportionate improvement on numerical questions. Self-correction shows modest semantic similarity gains but substantial numeric accuracy improvement, demonstrating it specifically fixes hard numerical extraction problems.}
\label{tab:disproportionate-gain}
\end{table}

This pattern is significant: the +XX\% numeric accuracy improvement versus +X\% semantic similarity gain proves that self-correction is not merely ``better prompting''---it specifically addresses the failure modes (table extraction, unit interpretation, temporal alignment) that cause numerical errors.

\subsection{Life of a Question: Correction Flow Visualization}
\label{sec:sankey}

Figure~\ref{fig:correction-sankey} visualizes the complete ``life of a question'' through our self-correction pipeline, making the correction dynamics concrete.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.2cm and 2.5cm,
    box/.style={draw, rounded corners, minimum width=2.2cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={-Stealth, thick},
    label/.style={font=\scriptsize, align=center}
]
% Attempt 1 outcomes
\node[box, fill=green!20] (correct1) {Correct\\(XX)};
\node[box, fill=red!20, below=of correct1] (wrong1) {Incorrect\\(XX)};

% Judge decisions
\node[box, fill=orange!20, right=of wrong1, yshift=0.6cm] (flagged) {Flagged\\(XX)};
\node[box, fill=gray!20, right=of wrong1, yshift=-0.6cm] (missed) {Missed\\(XX)};

% Final outcomes
\node[box, fill=green!30, right=of flagged, yshift=0.6cm] (corrected) {Corrected\\(XX)};
\node[box, fill=red!30, right=of flagged, yshift=-0.6cm] (stillwrong) {Still Wrong\\(XX)};

% Labels
\node[above=0.3cm of correct1, font=\small\bfseries] {Attempt 1};
\node[above=0.3cm of flagged, font=\small\bfseries] {Judge};
\node[above=0.3cm of corrected, font=\small\bfseries] {Final};

% Arrows with counts
\draw[arrow, green!60!black] (correct1) -- ++(3.5,0) node[midway, above, label] {Pass through};
\draw[arrow, orange!70!black] (wrong1) -- (flagged) node[midway, above, label] {XX\%};
\draw[arrow, gray!60!black] (wrong1) -- (missed) node[midway, below, label] {XX\%};
\draw[arrow, green!60!black] (flagged) -- (corrected) node[midway, above, label] {\textbf{Lazarus}\\XX\%};
\draw[arrow, red!60!black] (flagged) -- (stillwrong) node[midway, below, label] {XX\%};

% Summary box
\node[draw, dashed, rounded corners, fit=(correct1)(wrong1), inner sep=0.3cm, label={[font=\scriptsize]below:150 questions}] {};

\end{tikzpicture}
\caption{Correction flow visualization (``Life of a Question''). Starting from Attempt~1 outcomes, questions flow through the Judge Agent which decides whether to retry. The \textbf{Lazarus Rate} (XX\%) represents the proportion of initially incorrect answers successfully corrected---the core measure of system resilience.}
\label{fig:correction-sankey}
\end{figure}

The visualization makes three key points explicit: (1)~most questions are answered correctly on the first attempt, minimizing retry overhead; (2)~the Judge successfully identifies most failures (high true positive rate); (3)~retry successfully corrects a substantial portion of flagged questions (the Lazarus Rate).

\subsection{Forensic Case Study: Judge Semantic Understanding}
\label{sec:case-study}

To demonstrate the Judge Agent's reasoning capabilities, we present a representative example where the Judge correctly identified a subtle temporal mismatch error.

\begin{tcolorbox}[
    colback=blue!3!white,
    colframe=blue!50!black,
    title=\textbf{Case Study: Temporal Mismatch Detection},
    fonttitle=\small,
    arc=2mm,
    boxrule=0.5pt
]
\small
\textbf{Question:} What was Apple's gross margin in FY2022?

\textbf{Attempt 1 Answer:} ``43.3\%''

\textbf{Retrieved Context:} \textit{``...gross margin of 43.3\% for fiscal year 2021...''}

\textbf{Judge Response:}
\begin{verbatim}
{
  "score": 0.25,
  "reasoning": "Retrieved document mentions '43.3% gross
    margin' but for FY2021, not FY2022 as asked. The
    answer extracts a value from the wrong fiscal period.",
  "decision": "RETRY",
  "suggestion": "Re-retrieve with explicit FY2022 filter"
}
\end{verbatim}

\textbf{Attempt 2:} System retries with temporal filter $\rightarrow$ Answer: ``43.3\%'' (FY2022)

\textbf{Ground Truth:} 43.3\% (FY2022) $\checkmark$

\medskip
\textit{Analysis:} The initial answer was numerically plausible but from the wrong year---a subtle error that would pass surface-level validation. The Judge detected the temporal mismatch between the question (FY2022) and the retrieved context (FY2021), triggering retry with explicit year filtering.
\end{tcolorbox}

This case illustrates the Judge's value: it catches errors that require \emph{semantic understanding} of the relationship between question intent, retrieved context, and generated answer---exactly the verification a human analyst would perform.

\subsection{Routing Efficiency Validation}
\label{sec:routing-validation}

A potential concern is whether our rule-based routing (Section~\ref{sec:method}) sacrifices accuracy for simplicity. Table~\ref{tab:routing-comparison} compares rule-based routing against an LLM-based alternative.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Router Type} & \textbf{Routing Acc.} & \textbf{Latency} & \textbf{Cost/Query} \\
\midrule
Rule-Based (Ours) & 0.XX & $<$10ms & \$0.00 \\
LLM Router (GPT-4o-mini) & 0.XX & $\sim$500ms & $\sim$\$0.001 \\
\bottomrule
\end{tabular}
\caption{Routing efficiency comparison. Rule-based routing achieves comparable accuracy with zero latency overhead and no API cost.}
\label{tab:routing-comparison}
\end{table}

The rule-based router achieves XX\% of LLM router accuracy while adding zero latency and zero cost per query. For our target use case (financial analyst support tool), this tradeoff is favorable: the marginal accuracy gain from LLM routing does not justify the latency and cost overhead, especially since the Judge Agent provides a second opportunity to detect and correct errors.

\paragraph{Routing Decision Distribution.}
On FinanceBench, the rule-based router selects strategies as follows:
\begin{itemize}[nosep]
    \item \textbf{Hybrid + Rerank}: XX\% of questions (general financial QA)
    \item \textbf{Semantic-only}: XX\% of questions (conceptual/definitional)
    \item \textbf{Filtered + Rerank}: XX\% of questions (company/year-specific)
\end{itemize}

This distribution aligns with FinanceBench's question type composition, suggesting the routing heuristics correctly identify question characteristics.

\subsection{Robustness Validation: Cross-Domain Generalization}

To validate that our approach is not overfit to financial documents, we evaluate on two additional high-stakes domains \emph{without architecture modifications}:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Domain} & \textbf{Single-Pass} & \textbf{Multi-Agent} \\
\midrule
PubMedQA (Biomedical) & 0.XX & 0.XX \\
CUAD (Legal) & 0.XX & 0.XX \\
\bottomrule
\end{tabular}
\caption{Cross-domain validation. Same architecture achieves consistent improvements across fundamentally different document types.}
\label{tab:cross-domain}
\end{table}

The consistent improvements demonstrate architectural robustness: the same self-correction mechanism that works for SEC filing numerical extraction also works for biomedical yes/no reasoning and legal clause extraction, indicating resilience to distribution shifts, a critical property for financial AI systems that must handle diverse document sources.

\subsection{Failure Mode Analysis}

To understand system limitations, we analyze cases where Self-Correcting RAG fails to improve over single-pass baselines:

\begin{itemize}[nosep]
    \item \textbf{Retrieval ceiling} (X\% of failures): When relevant information is absent from the document corpus, escalated retrieval cannot recover; the system correctly identifies low confidence but cannot improve the answer
    \item \textbf{Arithmetic errors} (X\% of failures): Multi-step calculations (e.g., computing profit margins, growth rates) accumulate rounding errors or apply incorrect formulas, particularly when intermediate values must be derived from retrieved figures
    \item \textbf{Unit/scale confusion} (X\% of failures): Misinterpreting units (thousands vs.\ millions) or mixing absolute values with percentages leads to answers off by orders of magnitude, a critical error in financial contexts
    \item \textbf{Temporal misalignment} (X\% of failures): Extracting figures from incorrect fiscal periods or conflating FY with calendar year dates, especially prevalent in year-over-year comparison questions
    \item \textbf{Judge miscalibration} (X\% of failures): The Judge Agent scores an incorrect answer highly, preventing beneficial retry; occurs more frequently on plausible-sounding but numerically incorrect responses
    \item \textbf{Hallucinated figures} (X\% of failures): The model generates specific numerical values not present in retrieved context, often plausible-looking figures that pass surface-level validation
\end{itemize}

These failure modes suggest future work on retrieval source expansion (web search fallback), numerical verification chains, and judge calibration for domain-specific accuracy.

\begin{tcolorbox}[
    colback=red!3!white,
    colframe=red!40!black,
    title=\textbf{Example: Unit/Scale Confusion Error},
    fonttitle=\small,
    arc=2mm,
    boxrule=0.5pt
]
\small
\textbf{Question:} What was Company X's total revenue for FY2023?

\textbf{Retrieved Context:} ``...total revenues of \$X,XXX for the fiscal year ended December 31, 2023 (in millions)...''

\textbf{Model Answer (Initial):} \$X,XXX

\textbf{Ground Truth:} \$X.X billion

\medskip
\textit{Analysis:} The model correctly extracted the numeric value but failed to apply the ``in millions'' unit qualifier, producing an answer off by a factor of 1,000. The Judge Agent detected this inconsistency (score: X.X/10) and triggered retry. On the second attempt, the model correctly interpreted the unit context and produced the answer in billions.
\end{tcolorbox}

\subsection{Efficiency}

\paragraph{Deployment Context.}
This system is designed as an \emph{analyst support tool} for financial research, not a real-time chatbot. Response times of 10--30 seconds are acceptable in contexts where analysts currently spend minutes manually searching SEC filings. In high-stakes finance, the cost of returning an incorrect answer (e.g., misstated revenue leading to flawed investment decisions) far outweighs the latency cost of verification. For time-critical applications, the retry mechanism can be disabled, reverting to single-pass behavior.

Self-Correcting RAG introduces overhead from multiple agent calls and potential retries:

\begin{itemize}[nosep]
    \item \textbf{Single-pass latency}: $\sim$X seconds per question
    \item \textbf{Multi-Agent (no retry)}: $\sim$X seconds per question (+X\%)
    \item \textbf{Multi-Agent (with retry)}: $\sim$X seconds per question (when retry triggered)
\end{itemize}

The latency increase is acceptable given the reliability improvements, especially in high-stakes financial domains where correctness matters more than speed.
