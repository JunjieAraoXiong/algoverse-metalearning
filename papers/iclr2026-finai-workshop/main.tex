% Multi-Agent RAG: Self-Correcting Retrieval for Financial Document QA
% FINAI@ICLR 2026 Workshop Paper

\documentclass{article}
\usepackage{iclr2026_conference,times}

% Required packages
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage[most]{tcolorbox}

% Title
\title{Self-Correcting RAG: Judge-Driven Retrieval \\ for Financial Document QA}

% Authors - hidden for anonymous submission
\author{Anonymous Authors}

% Uncomment for camera-ready version
% \iclrfinalcopy

\begin{document}

\maketitle

\begin{abstract}
Deploying retrieval-augmented generation (RAG) in high-stakes finance requires two non-negotiable attributes: \textbf{numerical precision} in extraction and \textbf{audit trails} for regulatory compliance. Standard single-pass systems fail on both counts, frequently hallucinating figures while providing no transparent decision trace. We present Self-Correcting RAG, a framework that decomposes document QA into three specialized agents (Retrieval, Reasoning, and Judge) coordinated by an orchestrator with feedback-driven self-correction. When the Judge Agent scores an answer below a dynamic threshold, the system triggers retry with escalated strategies: broader retrieval, more careful prompting, and relaxed acceptance criteria. On FinanceBench (SEC filing QA), our approach recovers from failures that single-pass systems cannot detect, with largest gains on metrics-generated questions requiring numerical extraction. A key finding is that simple rule-based routing with judge-driven retry matches learned router performance while providing full interpretability. Every decision is logged with confidence scores, enabling the audit trails required for regulated financial applications. Cross-domain validation on medical (PubMedQA) and legal (CUAD) benchmarks confirms architectural robustness without domain-specific tuning.
\end{abstract}

% Include sections
\input{intro}
\input{related}
\input{method}
\input{experiments}
\input{results}
\input{conclusion}

% Appendix: LLM Usage Disclosure
\appendix
\section*{Statement on LLM Usage}

In accordance with ICLR's policy on Large Language Models, we declare:

\textbf{Text Refinement:} LLMs assisted with grammar and clarity improvements.

\textbf{Citation Verification:} We developed an automated citation verification
agent that cross-referenced all claims against source abstracts via Semantic
Scholar to ensure citation accuracy.

All technical contributions, experimental results, and analysis were conceived
and verified by the authors.

% Bibliography
\bibliography{references}
\bibliographystyle{iclr2026_conference}

\end{document}
