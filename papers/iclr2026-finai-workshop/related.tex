% Related Work Section for Multi-Agent RAG Paper
% FINAI@ICLR 2026 Workshop

\section{Related Work}
\label{sec:related}

Our work falls within the emerging paradigm of \textit{Agentic RAG}~\citep{singh2025agenticrag}, which embeds autonomous AI agents into retrieval-augmented pipelines to enable reflection, planning, and self-correction capabilities beyond traditional single-pass systems.

\paragraph{Self-Correction in Language Models.}
Iterative self-correction has emerged as a powerful paradigm for improving LLM outputs. Reflexion~\citep{shinn2023reflexion} introduces \textit{verbal reinforcement learning}, where agents convert scalar feedback into natural language reflections stored in episodic memory, achieving significant gains across decision-making (+22\% on AlfWorld), reasoning (+20\% on HotPotQA), and programming tasks. Self-Refine~\citep{madaan2023self} achieves 15--20\% improvements through single-model generate-critique-refine loops. In the RAG setting, Self-RAG~\citep{asai2024selfrag} trains models to generate retrieval tokens and self-assess relevance, and CRAG~\citep{yan2024corrective} triggers corrective retrieval when initial results are insufficient. Importantly, CRAG's fallback to external web search is incompatible with proprietary financial environments where data exfiltration is prohibited and answers must be grounded exclusively in authorized documents. Our framework is designed for these \textbf{``walled garden''} deployments: all escalation strategies operate \emph{within} the secure corpus, making it suitable for regulated financial institutions where external data access is forbidden.

Our approach differs from each of these methods in key ways. Unlike Reflexion, which maintains an episodic memory buffer of verbal reflections across multiple trials (typically 3--12 episodes) to iteratively refine a policy, our method performs self-correction \emph{within a single query session}: the Judge Agent's feedback triggers immediate retrieval escalation rather than being stored for future trials. This design reflects the real-time nature of financial QA, where users expect answers within seconds, not across multiple attempts. Our Judge Agent mirrors Reflexion's Evaluator component (both convert task performance into actionable signals) but operates at the \emph{retrieval-generation} boundary rather than the \emph{trial-episode} boundary, enabling correction before an answer is finalized rather than after failure. Whereas Self-Refine uses a single model as both critic and generator, we adopt a \emph{multi-agent approach with specialized components} for retrieval, generation, and evaluation. This division enables domain-specific modules (e.g., financial entity extractors) without retraining. Unlike Self-RAG, which integrates retrieval and reflection into a single model via special training tokens, our approach keeps retrieval and reflection as \emph{separate, modular agents}, avoiding custom training and allowing plug-and-play integration of powerful domain-specific retrievers. Recent work on Multi-Agent Reflexion~\citep{ozer2024mar} demonstrates that multi-agent critique outperforms single-agent self-reflection by providing diverse feedback perspectives and avoiding confirmation bias, supporting our architectural choice of agent separation.

These methods address \emph{what} to correct but leave open \emph{how} to correct in document-grounded QA: when retrieval fails, should the system retrieve more documents, use different strategies, or adjust generation prompts? Our contribution is orthogonal: we introduce explicit escalation policies across retrieval, generation, and evaluation stages, with a Judge Agent making discrete ``retry or accept'' decisions, particularly valuable in finance where the \emph{cost} of errors justifies multi-stage verification.

\paragraph{Adaptive Retrieval and Query Routing.}
Recent work explores adapting retrieval strategies to query characteristics. Adaptive-RAG~\citep{jeong2024adaptive} uses a classifier to route queries to different retrieval pipelines based on complexity. RouteRAG employs reinforcement learning to select retrieval strategies dynamically. RAGRouter~\citep{zhang2025ragrouter} extends this by routing across heterogeneous RAG systems, a complementary challenge to our focus on self-correction within a unified multi-agent system. Oracle routing experiments demonstrate that optimal per-query pipeline selection can significantly outperform fixed configurations.

Unlike these approaches that focus solely on \emph{initial} pipeline selection, our framework combines routing with \emph{judge-driven retry}: if the first attempt fails, the system escalates rather than returning a low-quality answer. This ``route + retry'' strategy recovers from routing errors that single-shot classifiers cannot self-correct. A key finding of our work is that \emph{simple rule-based routing with judge-driven retry achieves comparable performance to learned routers}, without training overhead, with full interpretability, and with explicit audit trails. For financial applications where model decisions must be explainable, this represents a practical advantage over black-box routing learned via RL.

\paragraph{Document QA and Financial Benchmarks.}
Standard RAG pipelines retrieve documents once and generate a single answer~\citep{lewis2020retrieval}. While advances in hybrid retrieval~\citep{chen2024bge}, cross-encoder reranking~\citep{nogueira2019passage}, and query expansion~\citep{gao2022precise} improve single-pass performance, they cannot recover when initial retrieval misses critical context. FinanceBench~\citep{islam2023financebench} reveals this limitation: questions requiring numerical precision from SEC filings often fail on first retrieval due to entity ambiguity or temporal misalignment.

Our approach treats single-pass RAG as a \emph{first attempt}, not the final answer. When the Judge Agent detects low confidence, escalated retrieval and generation strategies provide systematic recovery, addressing the reliability gap that makes single-pass RAG unsuitable for high-stakes financial applications.

\paragraph{LLM-as-Judge and Verification.}
Using LLMs to evaluate generated text provides scalable quality assessment~\citep{zheng2023judging}. G-Eval~\citep{liu2023geval} applies chain-of-thought for fine-grained evaluation. In our framework, the Judge Agent serves dual purposes: (1)~triggering retry when quality is insufficient, and (2)~providing confidence scores for downstream decision-making. This ``reject option'' (abstaining or flagging uncertainty rather than returning unreliable answers) aligns with responsible AI requirements in regulated industries where false confidence carries real costs.
