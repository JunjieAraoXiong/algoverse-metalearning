{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Ablation Study: Financial Question Answering\n",
        "\n",
        "## Publication-Quality Analysis\n",
        "\n",
        "This notebook analyzes results from a comprehensive experiment suite covering:\n",
        "- **25 total experiments**\n",
        "- Chunking strategies (standard vs element-based)\n",
        "- Chunk sizes (500-3000 characters)\n",
        "- Retrieval k values (3-30)\n",
        "- Retrieval enhancements (hybrid, metadata, reranking)\n",
        "- Embedding models (small vs large)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from scipy import stats\n",
        "\n",
        "# Set publication style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "\n",
        "RESULTS_DIR = Path('/Users/hansonxiong/Desktop/algoverse/LLM_as_a_Judge/experiments/results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load all experiment results\n",
        "def load_all_results():\n",
        "    results = []\n",
        "    for csv_file in RESULTS_DIR.glob('*.csv'):\n",
        "        if '_with_judge' in csv_file.name or csv_file.name.startswith(('baseline', 'ablation', 'chunk', 'k_value', 'embedding', 'optimal')):\n",
        "            df = pd.read_csv(csv_file)\n",
        "            exp_name = csv_file.stem.split('_2')[0]  # Remove timestamp\n",
        "            \n",
        "            # Normalize score column\n",
        "            score_col = 'judge_score' if 'judge_score' in df.columns else 'score'\n",
        "            if score_col in df.columns:\n",
        "                scores = pd.to_numeric(df[score_col], errors='coerce').dropna()\n",
        "                results.append({\n",
        "                    'experiment': exp_name,\n",
        "                    'mean_score': scores.mean(),\n",
        "                    'std_score': scores.std(),\n",
        "                    'median_score': scores.median(),\n",
        "                    'n_questions': len(scores),\n",
        "                    'perfect_scores': (scores == 1.0).sum(),\n",
        "                    'zero_scores': (scores == 0.0).sum(),\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(results).sort_values('mean_score', ascending=False)\n",
        "\n",
        "results_df = load_all_results()\n",
        "print(f'Loaded {len(results_df)} experiments')\n",
        "results_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 1: Overall Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Publication-quality bar chart\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "colors = plt.cm.RdYlGn(results_df['mean_score'].values)\n",
        "\n",
        "bars = ax.barh(results_df['experiment'], results_df['mean_score'] * 100, \n",
        "               xerr=results_df['std_score'] * 100, \n",
        "               color=colors, capsize=3)\n",
        "\n",
        "ax.set_xlabel('Mean Judge Score (%)')\n",
        "ax.set_title('RAG Configuration Performance Comparison')\n",
        "ax.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'fig1_overall_comparison.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(RESULTS_DIR / 'fig1_overall_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 2: Ablation Study Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter ablation experiments\n",
        "ablation_df = results_df[results_df['experiment'].str.contains('ablation|baseline')].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = range(len(ablation_df))\n",
        "bars = ax.bar(x, ablation_df['mean_score'] * 100, \n",
        "              yerr=ablation_df['std_score'] * 100,\n",
        "              color=plt.cm.viridis(np.linspace(0.3, 0.9, len(ablation_df))),\n",
        "              capsize=5)\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(ablation_df['experiment'].str.replace('ablation_', '').str.replace('baseline_', ''), \n",
        "                   rotation=45, ha='right')\n",
        "ax.set_ylabel('Mean Judge Score (%)')\n",
        "ax.set_title('Retrieval Enhancement Ablation Study')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, ablation_df['mean_score'] * 100):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
        "            f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'fig2_ablation_study.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(RESULTS_DIR / 'fig2_ablation_study.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 3: Chunk Size Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter chunk experiments\n",
        "chunk_df = results_df[results_df['experiment'].str.contains('chunk_')].copy()\n",
        "\n",
        "# Extract chunk size and type\n",
        "chunk_df['chunk_size'] = chunk_df['experiment'].str.extract(r'(\\d+)').astype(int)\n",
        "chunk_df['chunk_type'] = chunk_df['experiment'].apply(\n",
        "    lambda x: 'Element-based' if 'element' in x else 'Standard'\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for chunk_type, group in chunk_df.groupby('chunk_type'):\n",
        "    group = group.sort_values('chunk_size')\n",
        "    ax.errorbar(group['chunk_size'], group['mean_score'] * 100, \n",
        "                yerr=group['std_score'] * 100,\n",
        "                marker='o', markersize=8, capsize=5, label=chunk_type)\n",
        "\n",
        "ax.set_xlabel('Chunk Size (characters)')\n",
        "ax.set_ylabel('Mean Judge Score (%)')\n",
        "ax.set_title('Effect of Chunk Size on Performance')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'fig3_chunk_size.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(RESULTS_DIR / 'fig3_chunk_size.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figure 4: K-Value Sensitivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter k-value experiments\n",
        "k_df = results_df[results_df['experiment'].str.contains('k_value')].copy()\n",
        "k_df['k'] = k_df['experiment'].str.extract(r'(\\d+)').astype(int)\n",
        "k_df = k_df.sort_values('k')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.errorbar(k_df['k'], k_df['mean_score'] * 100, \n",
        "            yerr=k_df['std_score'] * 100,\n",
        "            marker='s', markersize=10, capsize=5, \n",
        "            color='#2ecc71', linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Top-K Retrieved Chunks')\n",
        "ax.set_ylabel('Mean Judge Score (%)')\n",
        "ax.set_title('Retrieval Depth Sensitivity Analysis')\n",
        "\n",
        "# Mark optimal k\n",
        "best_k = k_df.loc[k_df['mean_score'].idxmax(), 'k']\n",
        "ax.axvline(x=best_k, color='red', linestyle='--', alpha=0.5, \n",
        "           label=f'Optimal k={best_k}')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'fig4_k_sensitivity.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(RESULTS_DIR / 'fig4_k_sensitivity.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table 1: Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create publication-quality summary table\n",
        "summary_table = results_df.copy()\n",
        "summary_table['mean_score'] = (summary_table['mean_score'] * 100).round(1)\n",
        "summary_table['std_score'] = (summary_table['std_score'] * 100).round(1)\n",
        "summary_table['median_score'] = (summary_table['median_score'] * 100).round(1)\n",
        "\n",
        "summary_table = summary_table.rename(columns={\n",
        "    'experiment': 'Configuration',\n",
        "    'mean_score': 'Mean (%)',\n",
        "    'std_score': 'Std (%)',\n",
        "    'median_score': 'Median (%)',\n",
        "    'n_questions': 'N',\n",
        "    'perfect_scores': 'Perfect',\n",
        "    'zero_scores': 'Zero'\n",
        "})\n",
        "\n",
        "# Export to LaTeX for paper\n",
        "latex_table = summary_table.to_latex(index=False, \n",
        "                                     caption='RAG Configuration Performance Summary',\n",
        "                                     label='tab:results')\n",
        "\n",
        "with open(RESULTS_DIR / 'table1_summary.tex', 'w') as f:\n",
        "    f.write(latex_table)\n",
        "\n",
        "print('Top 10 Configurations:')\n",
        "summary_table.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Statistical significance tests\n",
        "print('Statistical Significance Tests')\n",
        "print('=' * 50)\n",
        "\n",
        "# Compare best vs baseline\n",
        "best_exp = results_df.iloc[0]['experiment']\n",
        "baseline_exp = results_df[results_df['experiment'].str.contains('baseline')].iloc[0]['experiment']\n",
        "\n",
        "print(f'\\nBest configuration: {best_exp}')\n",
        "print(f'Baseline: {baseline_exp}')\n",
        "print(f'\\nImprovement: {(results_df.iloc[0][\"mean_score\"] - results_df[results_df[\"experiment\"]==baseline_exp][\"mean_score\"].values[0]) * 100:.1f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}